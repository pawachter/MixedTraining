experiment: classification_DomainNet     # used to create experiment folder

data:                                 # ------ DATASET ------------------
  data_path_real : ./Data/domainNet/real
  data_path_synth: ./Data/domainNet/synth
  image_size     : [320, 320, 3]
  num_classes    : 60                

compile:                              # ------ GLOBAL COMPILE PARAMS ----
  metrics  : [accuracy, precision, recall, f1-score, top-3, top-5]
  loss     : categorical_crossentropy
  optimizer: sgd          
  lr       : 0.01                      

models:                               # ------ ARCHITECTURES ------------
  cnn:
    layers:
      - {type: conv2d, filters: 32, kernel_size: [3, 3], strides: [1, 1], padding: same, activation: relu}
      - {type: conv2d, filters: 32, kernel_size: [3, 3], strides: [1, 1], padding: same, activation: relu}
      - {type: maxpooling2d, pool_size: [2, 2], strides: [2, 2], padding: valid}
      - {type: conv2d, filters: 64, kernel_size: [3, 3], strides: [1, 1], padding: same, activation: relu}
      - {type: conv2d, filters: 64, kernel_size: [3, 3], strides: [1, 1], padding: same, activation: relu}
      - {type: maxpooling2d, pool_size: [2, 2], strides: [2, 2], padding: valid}
      - {type: conv2d, filters: 64, kernel_size: [3, 3], strides: [1, 1], padding: same, activation: relu}
      - {type: conv2d, filters: 64, kernel_size: [3, 3], strides: [1, 1], padding: same, activation: relu}
      - {type: maxpooling2d, pool_size: [2, 2], strides: [2, 2], padding: valid}
      - {type: flatten}
      - {type: dense, units: 128, activation: relu}
      - {type: dense, units: 64, activation: relu}
      - {type: dense, units: 60, activation: softmax}

  mlp:
    layers:
      - {type: flatten}
      - {type: dense, units: 512, activation: relu}
      - {type: dense, units: 512, activation: relu}
      - {type: dense, units: 256, activation: relu}
      - {type: dense, units: 256, activation: relu}
      - {type: dense, units: 128, activation: relu}
      - {type: dense, units: 60,  activation: softmax}

  vit:
    patch_size        : 32
    projection_dim    : 64
    num_heads         : 16
    transformer_layers: 8
    mlp_head_units    : [512, 256]

training:                             # ------ TRAINING -----------------
  num_epochs: 100
  batch_size: 64
  settings  : [simple_mixed, fine-tuned]
  patience  : 10
